{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.metrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Tiger annotated Stuttgart Tübingen tag set (STTS) as Universal tagset\n",
    "def read_stts_file_as_universal_tagset(path):\n",
    "    # Tagset: Stuttgart Tübingen tag set aka STTS to Universal\n",
    "    # Documentation: https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.mapping\n",
    "    # Input mapping page: https://github.com/slavpetrov/universal-pos-tags/blob/master/de-tiger.map\n",
    "    # Output mapping page: https://github.com/slavpetrov/universal-pos-tags/blob/master/universal_tags.py\n",
    "    universal_tagset = nltk.tag.tagset_mapping('de-tiger', 'universal') # NLTK <3\n",
    "\n",
    "    tagged_sentences = []\n",
    "    \n",
    "    with open(path) as file:\n",
    "        for index, sentence in enumerate(file):\n",
    "            tagged_sentences.append([])\n",
    "            for textblock in sentence.split(\";\"):\n",
    "                textblock = textblock.strip() # Trim leading or trailing whitespaces\n",
    "                touple = textblock.split(\"/\")\n",
    "                if(len(touple) > 1): # Check the length to prevent a crash at the beginning\n",
    "                    if (not textblock.startswith(\"//\")):\n",
    "                        tagged_sentences[index].append((touple[0], universal_tagset[touple[1]]))\n",
    "                    else:\n",
    "                        textvalue = \"\"\n",
    "                        textblock = textblock[2:] # Strip away the leading //\n",
    "                        \n",
    "                        if textblock == \"$,\":\n",
    "                            textvalue = \",\"\n",
    "                        elif textblock == \"$.\":\n",
    "                            textvalue = \".\"\n",
    "                        elif textblock == \"$(\":\n",
    "                            textvalue = \"-\"\n",
    "                        else:\n",
    "                            print(\"Malformed tag: \", textblock)\n",
    "\n",
    "                        tagged_sentences[index].append((textvalue, universal_tagset[textblock]))\n",
    "                #else:\n",
    "                    #print(\"Missed:\", index, len(touple), textblock)\n",
    "\n",
    "    return tagged_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences_train = read_stts_file_as_universal_tagset(\"POS_German_train.txt\")\n",
    "tagged_sentences_minitest = read_stts_file_as_universal_tagset(\"POS_German_minitest.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = int(len(tagged_sentences_train) * 0.9)\n",
    "real_train_sentences = tagged_sentences_train[:length]\n",
    "real_eval_sentences = tagged_sentences_train[length:]\n",
    "real_minitest = tagged_sentences_minitest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '.'), ('Ross', 'NOUN'), ('Perot', 'NOUN'), ('waere', 'VERB'), ('vielleicht', 'ADV'), ('ein', 'DET'), ('praechtiger', 'ADJ'), ('Diktator', 'NOUN'), (\"''\", '.')]\n",
      "[('Hinzu', 'PRT'), ('kommen', 'VERB'), ('Sprachprobleme', 'NOUN'), ('und', 'CONJ'), ('die', 'DET'), ('Unkenntnis', 'NOUN'), ('des', 'DET'), ('deutschen', 'ADJ'), ('Rechts', 'NOUN'), ('.', '.')]\n",
      "[('BONN', 'NOUN'), (',', '.'), ('10.', 'ADJ'), ('Maerz', 'NOUN'), ('(', '.'), ('dpa', 'NOUN'), (')', '.'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(real_train_sentences[0])\n",
    "print(real_eval_sentences[0])\n",
    "print(real_minitest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good & simple overview: https://www.kaggle.com/saxinou/nlp-02-categorizing-and-tagging-words\n",
    "def create_tagger(sentences):\n",
    "    default_tagger = nltk.DefaultTagger(\"NOUN\")\n",
    "    regexp_tagger = nltk.RegexpTagger([(r'^-?[0-9]+(.[0-9]+)?$', 'NUM')], backoff=default_tagger)\n",
    "    unigram_tagger = nltk.UnigramTagger(sentences, backoff=regexp_tagger)\n",
    "    bigram_tagger = nltk.BigramTagger(sentences, backoff=unigram_tagger)\n",
    "    trigram_tagger = nltk.BigramTagger(sentences, backoff=bigram_tagger)\n",
    "    return (default_tagger, regexp_tagger, unigram_tagger, bigram_tagger, trigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tagger(default_tagger, regexp_tagger, unigram_tagger, bigram_tagger, trigram_tagger, sentences):\n",
    "    print(\"Evaluate Default: \", default_tagger.evaluate(sentences))\n",
    "    print(\"Evaluate RegExp: \", regexp_tagger.evaluate(sentences))\n",
    "    print(\"Evaluate Unigram: \", unigram_tagger.evaluate(sentences))\n",
    "    print(\"Evaluate Bigram: \", bigram_tagger.evaluate(sentences))\n",
    "    print(\"Evaluate Trigram: \", trigram_tagger.evaluate(sentences))\n",
    "    return trigram_tagger.evaluate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(default_tagger, regexp_tagger, unigram_tagger, bigram_tagger, trigram_tagger, sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    print(\"Test Default: \", default_tagger.tag(tokens))\n",
    "    print(\"Test RegExp: \", regexp_tagger.tag(tokens))\n",
    "    print(\"Test Unigram: \", unigram_tagger.tag(tokens))\n",
    "    print(\"Test Bigram: \", bigram_tagger.tag(tokens))\n",
    "    print(\"Test Trigram: \", bigram_tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tagger, regexp_tagger, unigram_tagger, bigram_tagger, trigram_tagger = create_tagger(real_train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Default:  0.2728484715403799\n",
      "Evaluate RegExp:  0.2921247862979751\n",
      "Evaluate Unigram:  0.950985774111078\n",
      "Evaluate Bigram:  0.956051884498957\n",
      "Evaluate Trigram:  0.956051884498957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.956051884498957"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: Trigram to bigram is a 1/100 better\n",
    "evaluate_tagger(default_tagger, regexp_tagger, unigram_tagger, bigram_tagger, trigram_tagger, real_eval_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Default:  0.2765736684344017\n",
      "Evaluate RegExp:  0.28890938436707403\n",
      "Evaluate Unigram:  0.9468526631311966\n",
      "Evaluate Bigram:  0.9519252939820152\n",
      "Evaluate Trigram:  0.9519252939820152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9519252939820152"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: Trigram to bigram is only a 1/1000 better\n",
    "evaluate_tagger(default_tagger, regexp_tagger, unigram_tagger, bigram_tagger, trigram_tagger, real_minitest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Default:  [('Ich', 'NOUN'), ('trinke', 'NOUN'), ('immer', 'NOUN'), ('gerne', 'NOUN'), ('ein', 'NOUN'), ('kühles', 'NOUN'), ('Weizenbier', 'NOUN'), ('während', 'NOUN'), ('ich', 'NOUN'), ('auf', 'NOUN'), ('dem', 'NOUN'), ('Sofa', 'NOUN'), ('liege', 'NOUN'), ('und', 'NOUN'), ('Musik', 'NOUN'), ('höre', 'NOUN'), ('.', 'NOUN'), ('Auch', 'NOUN'), ('im', 'NOUN'), ('Jahre', 'NOUN'), ('2018', 'NOUN'), ('!', 'NOUN')]\n",
      "Test RegExp:  [('Ich', 'NOUN'), ('trinke', 'NOUN'), ('immer', 'NOUN'), ('gerne', 'NOUN'), ('ein', 'NOUN'), ('kühles', 'NOUN'), ('Weizenbier', 'NOUN'), ('während', 'NOUN'), ('ich', 'NOUN'), ('auf', 'NOUN'), ('dem', 'NOUN'), ('Sofa', 'NOUN'), ('liege', 'NOUN'), ('und', 'NOUN'), ('Musik', 'NOUN'), ('höre', 'NOUN'), ('.', 'NOUN'), ('Auch', 'NOUN'), ('im', 'NOUN'), ('Jahre', 'NOUN'), ('2018', 'NUM'), ('!', 'NOUN')]\n",
      "Test Unigram:  [('Ich', 'PRON'), ('trinke', 'VERB'), ('immer', 'ADV'), ('gerne', 'ADV'), ('ein', 'DET'), ('kühles', 'NOUN'), ('Weizenbier', 'NOUN'), ('während', 'NOUN'), ('ich', 'PRON'), ('auf', 'ADP'), ('dem', 'DET'), ('Sofa', 'NOUN'), ('liege', 'VERB'), ('und', 'CONJ'), ('Musik', 'NOUN'), ('höre', 'NOUN'), ('.', '.'), ('Auch', 'ADV'), ('im', 'ADP'), ('Jahre', 'NOUN'), ('2018', 'NUM'), ('!', '.')]\n",
      "Test Bigram:  [('Ich', 'PRON'), ('trinke', 'VERB'), ('immer', 'ADV'), ('gerne', 'ADV'), ('ein', 'DET'), ('kühles', 'NOUN'), ('Weizenbier', 'NOUN'), ('während', 'NOUN'), ('ich', 'PRON'), ('auf', 'ADP'), ('dem', 'DET'), ('Sofa', 'NOUN'), ('liege', 'VERB'), ('und', 'CONJ'), ('Musik', 'NOUN'), ('höre', 'NOUN'), ('.', '.'), ('Auch', 'ADV'), ('im', 'ADP'), ('Jahre', 'NOUN'), ('2018', 'NUM'), ('!', '.')]\n",
      "Test Trigram:  [('Ich', 'PRON'), ('trinke', 'VERB'), ('immer', 'ADV'), ('gerne', 'ADV'), ('ein', 'DET'), ('kühles', 'NOUN'), ('Weizenbier', 'NOUN'), ('während', 'NOUN'), ('ich', 'PRON'), ('auf', 'ADP'), ('dem', 'DET'), ('Sofa', 'NOUN'), ('liege', 'VERB'), ('und', 'CONJ'), ('Musik', 'NOUN'), ('höre', 'NOUN'), ('.', '.'), ('Auch', 'ADV'), ('im', 'ADP'), ('Jahre', 'NOUN'), ('2018', 'NUM'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Check: https://wortarten.info/\n",
    "sentence = \"Ich trinke immer gerne ein kühles Weizenbier während ich auf dem Sofa liege und Musik höre. Auch im Jahre 2018!\"\n",
    "tag_sentence(default_tagger, regexp_tagger, unigram_tagger, bigram_tagger, trigram_tagger, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(path):\n",
    "    tagged_sentences = read_stts_file_as_universal_tagset(path)\n",
    "\n",
    "    flatted_tags = [item for sublist in tagged_sentences for item in sublist]\n",
    "    tags = [touple[1] for touple in flatted_tags]\n",
    "\n",
    "    flatted_words = [item for sublist in tagged_sentences for item in sublist]\n",
    "    words = [touple[0] for touple in flatted_words]\n",
    "    \n",
    "    tagged_words = bigram_tagger.tag(words)\n",
    "    real_tagged_tags = [touple[1] for touple in tagged_words]\n",
    "\n",
    "    confusion_matrix = ConfusionMatrix(tags, real_tagged_tags)\n",
    "\n",
    "    print(\"Accuracy through evaluation: \", trigram_tagger.evaluate(tagged_sentences))\n",
    "    print(confusion_matrix.pretty_format(show_percents=True))\n",
    "    print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy through evaluation:  0.9519252939820152\n",
      "     |                                  C             N             P             V        |\n",
      "     |             A      A      A      O      D      O      N      R      P      E        |\n",
      "     |             D      D      D      N      E      U      U      O      R      R        |\n",
      "     |      .      J      P      V      J      T      N      M      N      T      B      X |\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "   . | <13.6%>     .      .      .      .      .      .      .      .      .      .      . |\n",
      " ADJ |      .  <6.3%>     .   0.1%      .      .   1.5%      .      .   0.0%   0.1%   0.0% |\n",
      " ADP |      .      . <11.6%>  0.0%   0.1%      .   0.0%      .      .   0.3%      .   0.0% |\n",
      " ADV |      .   0.0%   0.2%  <3.9%>  0.1%      .   0.0%      .   0.1%   0.0%      .   0.0% |\n",
      "CONJ |      .      .   0.1%   0.0%  <3.4%>     .      .      .   0.0%      .      .      . |\n",
      " DET |      .      .      .      .      . <10.3%>     .      .   0.3%      .      .      . |\n",
      "NOUN |      .   0.1%      .   0.0%      .   0.0% <27.5%>     .      .   0.0%      .   0.0% |\n",
      " NUM |      .      .      .      .      .   0.0%   0.0%  <1.9%>     .      .      .      . |\n",
      "PRON |      .   0.1%      .   0.0%      .   0.3%      .      .  <4.8%>     .   0.0%      . |\n",
      " PRT |      .      .   0.3%   0.0%      .   0.0%      .      .      .  <1.0%>     .      . |\n",
      "VERB |      .   0.1%      .      .      .      .   0.7%      .   0.0%      . <10.3%>     . |\n",
      "   X |      .      .      .      .   0.0%      .   0.1%      .      .      .      .  <0.6%>|\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |                        C         N         P         V      |\n",
      "     |         A    A    A    O    D    O    N    R    P    E      |\n",
      "     |         D    D    D    N    E    U    U    O    R    R      |\n",
      "     |    .    J    P    V    J    T    N    M    N    T    B    X |\n",
      "-----+-------------------------------------------------------------+\n",
      "   . |<1183>   .    .    .    .    .    .    .    .    .    .    . |\n",
      " ADJ |    . <546>   .   11    .    .  130    .    .    1   11    1 |\n",
      " ADP |    .    .<1003>   1    6    .    1    .    .   24    .    1 |\n",
      " ADV |    .    3   17 <335>   5    .    2    .    6    2    .    1 |\n",
      "CONJ |    .    .    7    4 <295>   .    .    .    2    .    .    . |\n",
      " DET |    .    .    .    .    . <895>   .    .   28    .    .    . |\n",
      "NOUN |    .    6    .    1    .    1<2388>   .    .    1    .    2 |\n",
      " NUM |    .    .    .    .    .    2    1 <161>   .    .    .    . |\n",
      "PRON |    .    5    .    3    .   23    .    . <414>   .    1    . |\n",
      " PRT |    .    .   27    1    .    2    .    .    .  <88>   .    . |\n",
      "VERB |    .    5    .    .    .    .   57    .    1    . <897>   . |\n",
      "   X |    .    .    .    .    1    .   12    .    .    .    .  <53>|\n",
      "-----+-------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(\"POS_German_minitest.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
